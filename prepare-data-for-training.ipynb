{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of routines for preparing data inside ML Studio for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "#todo: replace file name with the bigger one when script is ready\n",
    "sales =  pd.DataFrame.from_csv('PE-TRG-Jan-Mar-2017.csv').reset_index()\n",
    "\n",
    "#remove duplications by summing up quantity by days\n",
    "sales = pd.pivot_table(sales, values='Quantity', index=['Locationid','PLU','Year','Month','Day','WeekDay'], aggfunc=np.sum).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setCountGroup(row):\n",
    "    if row['counts'] < 10:\n",
    "        return \"1. less than 10\"\n",
    "    elif row['counts'] <= 20:\n",
    "        return \"2. 20 or less\"\n",
    "    elif row['counts'] <= 30:\n",
    "        return \"3. 30 or less\"\n",
    "    else: \n",
    "        return \"4. more than 30\"\n",
    "\n",
    "positions = sales.groupby(['Locationid','PLU']).size().rename('counts').reset_index()\n",
    "positions_count = len(positions.index)\n",
    "locations_count = len(positions.drop_duplicates(['Locationid']).index)\n",
    "products_count = len(positions.drop_duplicates(['PLU']).index)\n",
    "\n",
    "counts_groups = positions\n",
    "counts_groups['segment'] = counts_groups.apply(setCountGroup, axis=1)\n",
    "counts_groups = counts_groups.groupby('segment').size().rename('counts').reset_index()\n",
    "counts_groups['%'] = counts_groups.apply(lambda r: r['counts']/positions_count*100, axis=1)\n",
    "\n",
    "rows_total = len(sales.index)\n",
    "rows_with_negative = len(sales[sales['Quantity'] < 0].index)\n",
    "rows_with_zero = len(sales[sales['Quantity'] == 0].index)\n",
    "rows_with_positive = len(sales[sales['Quantity'] > 0].index)\n",
    "\n",
    "# ==== output results ====\n",
    "\n",
    "print(\"Total unique positions (Location-Recipe) %d\" % (positions_count))\n",
    "print(\"Total locations %d\" % (locations_count))\n",
    "print(\"Total recipes %d\" % (products_count))\n",
    "\n",
    "print(\"Data rows %d\" % (rows_total))\n",
    "print(\"Negative data %.2f%% (%d)\" % (rows_with_negative/rows_total*100, rows_with_negative))\n",
    "print(\"Zero data %.2f%% (%d)\" % (rows_with_zero/rows_total*100, rows_with_zero))\n",
    "print(\"Positive data %.2f%% (%d)\" % (rows_with_positive/rows_total*100, rows_with_positive))\n",
    "\n",
    "counts_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set zero values for negative quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fast version of assignment column value (instead of df.apply)\n",
    "sales.loc[sales['Quantity'] < 0, 'Quantity'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Saledate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_min = dt.date(sales['Year'].min(),sales['Month'].min(),sales['Day'].min())\n",
    "date_max = dt.date(sales['Year'].max(),sales['Month'].max(),sales['Day'].max())\n",
    "dates = pd.DataFrame(pd.date_range(date_min, date_max), columns=['Saledate'])\n",
    "dates['Year'] = dates['Saledate'].dt.year\n",
    "dates['Month'] = dates['Saledate'].dt.month\n",
    "dates['Day'] = dates['Saledate'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.merge(sales, dates, on=['Year','Month','Day'], suffixes=('', ''), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales = sales.reset_index()\n",
    "#del sales['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Feature: previous 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # disable SettingWithCopyWarning\n",
    "# slow version becasue it goes row by row\n",
    "# keep it for history as an example of the slowest approach\n",
    "def addPreviousDays(p, df, days):\n",
    "    locationid = p['Locationid']\n",
    "    plu = p['PLU']\n",
    "    date_finish = p['Saledate']\n",
    "    date_start = date_finish - dt.timedelta(days=days)\n",
    "    # filter sales by position and time frame in days\n",
    "    position_sales = _sales.loc[(_sales['Locationid']==locationid)\n",
    "                                &(_sales['PLU']==plu)\n",
    "                                &(_sales['Saledate']>=date_start)\n",
    "                                &(_sales['Saledate']<=date_finish)] # at least current row is required to provide the min date\n",
    "    min_date = position_sales['Saledate'].min()\n",
    "    \n",
    "    #add row with starting date if missing\n",
    "    if min_date > date_start:\n",
    "        new_row = pd.DataFrame([[locationid,plu,0,0,0,0,date_start]], columns=['Locationid','PLU','Year','Month','Day','Quantity','Saledate'])\n",
    "        position_sales = position_sales.append(new_row)\n",
    "    \n",
    "    position_sales = position_sales.reset_index().set_index('Saledate')\n",
    "    # Convert the integer timestamps in the index to a DatetimeIndex\n",
    "    position_sales.index = pd.to_datetime(position_sales.index)\n",
    "    \n",
    "    #resample data by days between starting and ending dates\n",
    "    qty_by_days = position_sales.resample('d').sum().reset_index()\n",
    "    del qty_by_days['index']\n",
    "    qty_by_days['Locationid'] = locationid\n",
    "    qty_by_days['PLU'] = plu\n",
    "    qty_by_days = qty_by_days.fillna(0)\n",
    "    \n",
    "    #transpose resampled data to get dates in columns\n",
    "    for index in range(1, days+1): # the index is moved +1 to skip the first date in resampled data\n",
    "        d = date_finish - dt.timedelta(days=index)\n",
    "        # take the value for day in the past\n",
    "        # df[].item() - this returns the first element in the Index/Series returned from that selection\n",
    "        dn = qty_by_days.loc[qty_by_days['Saledate']==d, 'Quantity'].item()\n",
    "        df.loc[(df['Locationid']==locationid)&(df['PLU']==plu)&(df['Saledate']==date_finish),'Day-'+str(index)] = dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return history of sales using resampling by days\n",
    "# slowest version\n",
    "def getSalesHistoryByDays(df, startDate):\n",
    "    #group by positions and set the starting date with zero quantity\n",
    "    #the date should be as earliest as possible to not overwrite quantity\n",
    "    startingPoint = df.groupby(['Locationid','PLU']).sum().reset_index()\n",
    "    startingPoint['Quantity'] = 0\n",
    "    startingPoint['Saledate'] = startDate\n",
    "\n",
    "    #merge position starting points with existing sales\n",
    "    sale_history = pd.concat([df, startingPoint])\n",
    "    sale_history = sale_history.reset_index().set_index('Saledate')\n",
    "    \n",
    "    #resemble history by groups starting from the start date\n",
    "    sale_history.index = pd.to_datetime(sale_history.index)\n",
    "    # (!) the slowest part of the execution\n",
    "    sale_history = sale_history.groupby(['Locationid','PLU']).resample('d').sum()\n",
    "    \n",
    "    #remove duplicated columns\n",
    "    del sale_history['PLU']\n",
    "    del sale_history['Locationid']\n",
    "    sale_history = sale_history.fillna(0)\n",
    "    \n",
    "    return sale_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return history of sales using resampling by days\n",
    "def getSalesHistoryByDays2(df, startDate):\n",
    "    \n",
    "    sale_history = df\n",
    "    \n",
    "    #init buffer for missing dates\n",
    "    missing_dates = df.groupby(['Locationid','PLU']).size().reset_index()[['Locationid','PLU']] #size() is used becasue it's fast\n",
    "    missing_dates['Saledate'] = dt.datetime.now()\n",
    "    missing_dates = missing_dates[missing_dates['Locationid'] == -1] # selecting nothing to remove rows\n",
    "    \n",
    "    # create a buffer with all required dates for all positions\n",
    "    # simply going by dates and accumulate in one dataframe with all required dates\n",
    "    for index, row in dates.iterrows():\n",
    "        date = row['Saledate']\n",
    "        # create new dataframe to set the next date\n",
    "        positions = df.groupby(['Locationid','PLU']).size().reset_index()\n",
    "        positions['Quantity'] = 0\n",
    "        positions['Saledate'] = date\n",
    "\n",
    "        missing_dates = pd.concat([missing_dates, positions])\n",
    "    \n",
    "    # concat existing sales with rows with missing dates\n",
    "    sale_history = pd.concat([sale_history, missing_dates])\n",
    "    sale_history = sale_history.reset_index().set_index('Saledate')\n",
    "    \n",
    "    del sale_history['index']\n",
    "    sale_history = sale_history.fillna(0)\n",
    "    \n",
    "    sale_history = sale_history.groupby(['Locationid','PLU','Saledate']).sum().reset_index()\n",
    "    \n",
    "    return sale_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_min = sales['Saledate'].min()\n",
    "# 1 loop, best of 3: 1min 51s per loop\n",
    "history = getSalesHistoryByDays2(sales, date_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add one day to the row from the past\n",
    "# quick version\n",
    "def addPreviousDay(sales, salesHistory, offsetToPastInDays):\n",
    "    #definenew key in history frame for later merge\n",
    "    shiftedHistory = salesHistory\n",
    "    shiftedHistory = shiftedHistory.reset_index()\n",
    "    shiftedHistory['D'] = shiftedHistory['Saledate']\n",
    "    \n",
    "    #define shifted date-key for merging past history to the same date entry\n",
    "    df = sales\n",
    "    df['D'] = df['Saledate'] - dt.timedelta(days=offsetToPastInDays)\n",
    "    \n",
    "    #set indexes\n",
    "    shiftedHistory = shiftedHistory.reset_index().set_index(['Locationid','PLU','D'])\n",
    "    df = df.reset_index().set_index(['Locationid','PLU','D'])\n",
    "    \n",
    "    # merge with history\n",
    "    df = pd.merge(shiftedHistory, df, left_index=True, right_index=True, suffixes=('_d', ''), how='inner')\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # cleanup the frame\n",
    "    df = df.drop(['D','level_0','Saledate_d','index_d','Year_d','Month_d','Day_d','index'], axis=1)\n",
    "    df = df.rename(columns={'Quantity_d':'Day-'+str(offsetToPastInDays)})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add several days from the past\n",
    "def addPreviousDays(sales, salesHistory, offsetToPastInDays):\n",
    "    s = sales\n",
    "    for index in range(1, offsetToPastInDays+1):\n",
    "        s = addPreviousDay(s, salesHistory, index)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_with_history = addPreviousDays(sales, history, 7)\n",
    "sales_with_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit addPreviousDays(sales, history, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#take the previous value to add more information (shift - move pointer backward)\n",
    "qty_by_days['Day-1'] = qty_by_days['Quantity'].shift(1)\n",
    "qty_by_days['Day-2'] = qty_by_days['Quantity'].shift(2)\n",
    "qty_by_days['Day-3'] = qty_by_days['Quantity'].shift(3)\n",
    "qty_by_days['Day-4'] = qty_by_days['Quantity'].shift(4)\n",
    "qty_by_days['Day-5'] = qty_by_days['Quantity'].shift(5)\n",
    "qty_by_days['Day-6'] = qty_by_days['Quantity'].shift(6)\n",
    "qty_by_days['Day-7'] = qty_by_days['Quantity'].shift(7)\n",
    "#replace NaN with zeros\n",
    "qty_by_days.fillna(0,inplace=True)\n",
    "\n",
    "#take weeks averages\n",
    "def week_avg(df, newColumn, startIndex):\n",
    "    df[newColumn] = (np.nan_to_num(df['Quantity'].shift(startIndex))\n",
    "                         +np.nan_to_num(df['Quantity'].shift(startIndex+1))\n",
    "                         +np.nan_to_num(df['Quantity'].shift(startIndex+2))\n",
    "                         +np.nan_to_num(df['Quantity'].shift(startIndex+3))\n",
    "                         +np.nan_to_num(df['Quantity'].shift(startIndex+4))\n",
    "                         +np.nan_to_num(df['Quantity'].shift(startIndex+5))\n",
    "                         +np.nan_to_num(df['Quantity'].shift(startIndex+6)))/7\n",
    "    \n",
    "week_avg(qty_by_days, 'Week-1', 1)\n",
    "week_avg(qty_by_days, 'Week-2', 2)\n",
    "week_avg(qty_by_days, 'Week-3', 3)\n",
    "week_avg(qty_by_days, 'Week-4', 4)\n",
    "week_avg(qty_by_days, 'Week-5', 5)\n",
    "\n",
    "qty_by_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qty_by_days.corr()['Quantity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qty_by_days[qty_by_days['PLU'] == 1894].to_csv('1894.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import pylab as plt\n",
    "\n",
    "X = qty_by_days['Quantity']\n",
    "Y1 = qty_by_days['Day-1']\n",
    "Y2 = qty_by_days['WeekDay']\n",
    "\n",
    "#plt.scatter(X,Y1,color='k', s=2)\n",
    "plt.scatter(X,Y2,color='g', s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
