{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of routines for preparing data inside ML Studio for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "#todo: replace file name with the bigger one when script is ready\n",
    "sales =  pd.DataFrame.from_csv('PE-TRG-Jan-Mar-2017.csv').reset_index()\n",
    "\n",
    "#remove duplications by summing up quantity by days\n",
    "sales = pd.pivot_table(sales, values='Quantity', index=['Locationid','PLU','Year','Month','Day'], aggfunc=np.sum).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setCountGroup(row):\n",
    "    if row['counts'] < 10:\n",
    "        return \"1. less than 10\"\n",
    "    elif row['counts'] <= 20:\n",
    "        return \"2. 20 or less\"\n",
    "    elif row['counts'] <= 30:\n",
    "        return \"3. 30 or less\"\n",
    "    else: \n",
    "        return \"4. more than 30\"\n",
    "\n",
    "positions = sales.groupby(['Locationid','PLU']).size().rename('counts').reset_index()\n",
    "positions_count = len(positions.index)\n",
    "locations_count = len(positions.drop_duplicates(['Locationid']).index)\n",
    "products_count = len(positions.drop_duplicates(['PLU']).index)\n",
    "\n",
    "counts_groups = positions\n",
    "counts_groups['segment'] = counts_groups.apply(setCountGroup, axis=1)\n",
    "counts_groups = counts_groups.groupby('segment').size().rename('counts').reset_index()\n",
    "counts_groups['%'] = counts_groups.apply(lambda r: r['counts']/positions_count*100, axis=1)\n",
    "\n",
    "rows_total = len(sales.index)\n",
    "rows_with_negative = len(sales[sales['Quantity'] < 0].index)\n",
    "rows_with_zero = len(sales[sales['Quantity'] == 0].index)\n",
    "rows_with_positive = len(sales[sales['Quantity'] > 0].index)\n",
    "\n",
    "# ==== output results ====\n",
    "\n",
    "print(\"Total unique positions (Location-Recipe) %d\" % (positions_count))\n",
    "print(\"Total locations %d\" % (locations_count))\n",
    "print(\"Total recipes %d\" % (products_count))\n",
    "\n",
    "print(\"Data rows %d\" % (rows_total))\n",
    "print(\"Negative data %.2f%% (%d)\" % (rows_with_negative/rows_total*100, rows_with_negative))\n",
    "print(\"Zero data %.2f%% (%d)\" % (rows_with_zero/rows_total*100, rows_with_zero))\n",
    "print(\"Positive data %.2f%% (%d)\" % (rows_with_positive/rows_total*100, rows_with_positive))\n",
    "\n",
    "counts_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set zero values for negative quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fast version of assignment column value (instead of df.apply)\n",
    "sales.loc[sales['Quantity'] < 0, 'Quantity'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatDate(r):\n",
    "    return dt.date(int(r['Year']), int(r['Month']), int(r['Day']))\n",
    "\n",
    "# (!) processing of 11002942 rows takes about 5 mins\n",
    "sales['Saledate'] = sales.apply(creatDate, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sales = sales.reset_index()\n",
    "del _sales['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Feature: previous 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # disable SettingWithCopyWarning\n",
    "# slow version becasue it goes row by row\n",
    "def addPreviousDays(p, df, days):\n",
    "    locationid = p['Locationid']\n",
    "    plu = p['PLU']\n",
    "    date_finish = p['Saledate']\n",
    "    date_start = date_finish - dt.timedelta(days=days)\n",
    "    # filter sales by position and time frame in days\n",
    "    position_sales = _sales.loc[(_sales['Locationid']==locationid)\n",
    "                                &(_sales['PLU']==plu)\n",
    "                                &(_sales['Saledate']>=date_start)\n",
    "                                &(_sales['Saledate']<=date_finish)] # at least current row is required to provide the min date\n",
    "    min_date = position_sales['Saledate'].min()\n",
    "    \n",
    "    #add row with starting date if missing\n",
    "    if min_date > date_start:\n",
    "        new_row = pd.DataFrame([[locationid,plu,0,0,0,0,date_start]], columns=['Locationid','PLU','Year','Month','Day','Quantity','Saledate'])\n",
    "        position_sales = position_sales.append(new_row)\n",
    "    \n",
    "    position_sales = position_sales.reset_index().set_index('Saledate')\n",
    "    # Convert the integer timestamps in the index to a DatetimeIndex\n",
    "    position_sales.index = pd.to_datetime(position_sales.index)\n",
    "    \n",
    "    #resample data by days between starting and ending dates\n",
    "    qty_by_days = position_sales.resample('d').sum().reset_index()\n",
    "    del qty_by_days['index']\n",
    "    qty_by_days['Locationid'] = locationid\n",
    "    qty_by_days['PLU'] = plu\n",
    "    qty_by_days = qty_by_days.fillna(0)\n",
    "    \n",
    "    #transpose resampled data to get dates in columns\n",
    "    for index in range(1, days+1): # the index is moved +1 to skip the first date in resampled data\n",
    "        d = date_finish - dt.timedelta(days=index)\n",
    "        # take the value for day in the past\n",
    "        # df[].item() - this returns the first element in the Index/Series returned from that selection\n",
    "        dn = qty_by_days.loc[qty_by_days['Saledate']==d, 'Quantity'].item()\n",
    "        df.loc[(df['Locationid']==locationid)&(df['PLU']==plu)&(df['Saledate']==date_finish),'Day-'+str(index)] = dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSalesHistoryByDays(df, startDate):\n",
    "    #group by positions and set the starting date with zero quantity\n",
    "    #the date should be as earliest as possible to not overwrite quantity\n",
    "    startingPoint = df.groupby(['Locationid','PLU']).sum().reset_index()\n",
    "    startingPoint['Quantity'] = 0\n",
    "    startingPoint['Saledate'] = startDate\n",
    "\n",
    "    #merge position starting points with existing sales\n",
    "    sale_history = pd.concat([df, startingPoint])\n",
    "    sale_history = sale_history.reset_index().set_index('Saledate')\n",
    "    #resemble history by groups starting from the start date\n",
    "    sale_history.index = pd.to_datetime(sale_history.index)\n",
    "    sale_history = sale_history.groupby(['Locationid','PLU']).resample('d').sum()\n",
    "    \n",
    "    #remove duplicated columns\n",
    "    del sale_history['PLU']\n",
    "    del sale_history['Locationid']\n",
    "    sale_history = sale_history.fillna(0)\n",
    "    \n",
    "    return sale_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = getSalesHistoryByDays(_sales.head(10000), dt.date(2016,1,1))\n",
    "#history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addPreviousDay(sales, salesHistory, offsetToPastInDays):\n",
    "    #definenew key in history frame for later merge\n",
    "    shiftedHistory = salesHistory\n",
    "    shiftedHistory = shiftedHistory.reset_index()\n",
    "    shiftedHistory['D'] = shiftedHistory['Saledate']\n",
    "    \n",
    "    #define shifted date-key for merging past history to the same date entry\n",
    "    df = sales\n",
    "    df['D'] = df['Saledate'] - dt.timedelta(days=offsetToPastInDays)\n",
    "    \n",
    "    #set indexes\n",
    "    shiftedHistory = shiftedHistory.reset_index().set_index(['Locationid','PLU','D'])\n",
    "    df = df.reset_index().set_index(['Locationid','PLU','D'])\n",
    "    \n",
    "    # merge with history\n",
    "    df = pd.merge(shiftedHistory, df, left_index=True, right_index=True, suffixes=('_d', ''), how='inner')\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # cleanup the frame\n",
    "    df = df.drop(['D','level_0','Saledate_d','index_d','Year_d','Month_d','Day_d','index'], axis=1)\n",
    "    df = df.rename(columns={'Quantity_d':'Day-'+str(offsetToPastInDays)})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = addPreviousDay(_sales.head(10000), history, 1)\n",
    "part = addPreviousDay(part, history, 2)\n",
    "part = addPreviousDay(part, history, 3)\n",
    "part = addPreviousDay(part, history, 4)\n",
    "part = addPreviousDay(part, history, 5)\n",
    "part = addPreviousDay(part, history, 6)\n",
    "part = addPreviousDay(part, history, 7)\n",
    "part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_rows = len(_sales.index)\n",
    "print(\"rows = %d\" % (total_rows))\n",
    "\n",
    "#part.apply(addPreviousDays, args=(part,7,), axis=1)\n",
    "\n",
    "# add Day-1\n",
    "d1 = history.reset_index()\n",
    "d1['D-1'] = d1['Saledate']\n",
    "d1 = d1.fillna(0)\n",
    "\n",
    "part = _sales.head(10000)\n",
    "part['D-1'] = part['Saledate'] - dt.timedelta(days=1)\n",
    "\n",
    "d1 = d1.reset_index().set_index(['Locationid','PLU','D-1'])\n",
    "part = part.reset_index().set_index(['Locationid','PLU','D-1'])\n",
    "\n",
    "part = pd.merge(d1, part, left_index=True, right_index=True, suffixes=('_d1', ''), how='inner')\n",
    "part = part.reset_index()\n",
    "\n",
    "part = part.drop(['D-1','level_0','Saledate_d1','index_d1','Year_d1','Month_d1','Day_d1','index'], axis=1)\n",
    "part = part.rename(columns={'Quantity_d1':'Day-1'})\n",
    "part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add Day-2\n",
    "d2 = history.reset_index()\n",
    "d2['D-2'] = d2['Saledate']\n",
    "d2 = d2.fillna(0)\n",
    "\n",
    "part['D-2'] = part['Saledate'] - dt.timedelta(days=2)\n",
    "d2 = d2.reset_index().set_index(['Locationid','PLU','D-2'])\n",
    "part = part.reset_index().set_index(['Locationid','PLU','D-2'])\n",
    "\n",
    "part = pd.merge(d2, part, left_index=True, right_index=True, suffixes=('_d2', ''), how='inner')\n",
    "part = part.reset_index()\n",
    "\n",
    "# remove columns\n",
    "#part.drop(['index_x','Year','Month','Day','level_0_x','Saledate','index_y','level_0_y','Saledate_x','Year_x','Month_x','Day_x'], axis=1)\n",
    "part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPreviousDays2(df, days):\n",
    "    # keep only existing sales for the period\n",
    "    #sales_in_period = df.loc[(_sales['Saledate']>=date_start)&(_sales['Saledate']<=date_finish)]\n",
    "    positions = df.groupby(['Locationid','PLU']).sum().reset_index()\n",
    "    positions.apply(df, dt.date(2016,1,1))\n",
    "    \n",
    "    df = df.reset_index().set_index('Saledate')\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df.groupby(['Locationid','PLU']).resample('d').sum()\n",
    "\n",
    "part = _sales.head(10)\n",
    "part.apply(addPreviousDays2, args=(7,), axis=1)\n",
    "part   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the previous value to add more information (shift - move pointer backward)\n",
    "qty_by_days['Day-1'] = qty_by_days['Quantity'].shift(1)\n",
    "qty_by_days['Day-2'] = qty_by_days['Quantity'].shift(2)\n",
    "qty_by_days['Day-3'] = qty_by_days['Quantity'].shift(3)\n",
    "qty_by_days['Day-4'] = qty_by_days['Quantity'].shift(4)\n",
    "qty_by_days['Day-5'] = qty_by_days['Quantity'].shift(5)\n",
    "qty_by_days['Day-6'] = qty_by_days['Quantity'].shift(6)\n",
    "qty_by_days['Day-7'] = qty_by_days['Quantity'].shift(7)\n",
    "#replace NaN with zeros\n",
    "qty_by_days.fillna(0,inplace=True)\n",
    "\n",
    "#take weeks averages\n",
    "def week_avg(df, newColumn, startIndex):\n",
    "    df[newColumn] = (np.nan_to_num(df['Quantity'].shift(startIndex))\n",
    "                         +np.nan_to_num(df['Quantity'].shift(startIndex+1))\n",
    "                         +np.nan_to_num(df['Quantity'].shift(startIndex+2))\n",
    "                         +np.nan_to_num(df['Quantity'].shift(startIndex+3))\n",
    "                         +np.nan_to_num(df['Quantity'].shift(startIndex+4))\n",
    "                         +np.nan_to_num(df['Quantity'].shift(startIndex+5))\n",
    "                         +np.nan_to_num(df['Quantity'].shift(startIndex+6)))/7\n",
    "    \n",
    "week_avg(qty_by_days, 'Week-1', 1)\n",
    "week_avg(qty_by_days, 'Week-2', 2)\n",
    "week_avg(qty_by_days, 'Week-3', 3)\n",
    "week_avg(qty_by_days, 'Week-4', 4)\n",
    "week_avg(qty_by_days, 'Week-5', 5)\n",
    "\n",
    "qty_by_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qty_by_days.corr()['Quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qty_by_days[qty_by_days['PLU'] == 1894].to_csv('1894.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "\n",
    "X = qty_by_days['Quantity']\n",
    "Y1 = qty_by_days['Day-1']\n",
    "Y2 = qty_by_days['WeekDay']\n",
    "\n",
    "#plt.scatter(X,Y1,color='k', s=2)\n",
    "plt.scatter(X,Y2,color='g', s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
